# 2.1 Review of Fundamental Hash Table Concepts

Hash tables are a fundamental data structure in computer science, providing a way to store key-value pairs and perform insertions, deletions, and lookups in average constant time, O(1). They are essential for building high-performance systems, but their classical implementation faces challenges when dealing with massive datasets.

## Core Concepts

A hash table is an array-based data structure that maps keys to values using a **hash function**. The hash function takes a key as input and computes an index (a "hash") into the array, which is where the corresponding value is stored.

### 1. Hash Function

A good hash function is the most critical component of a hash table. It should have the following properties:

*   **Deterministic:** The same key must always produce the same hash.
*   **Uniform Distribution:** It should distribute keys evenly across the array to minimize collisions.
*   **Fast to Compute:** The hash function itself should be efficient.

### 2. Collisions

A **collision** occurs when two different keys hash to the same index. Since the number of possible keys is usually much larger than the number of available array slots, collisions are inevitable. There are two primary strategies for handling them:

*   **Chaining:** Each array slot (or "bucket") points to a linked list of key-value pairs that have hashed to that index. When a collision occurs, the new pair is simply added to the linked list. This is the most common collision resolution strategy.
*   **Open Addressing:** If a collision occurs, the algorithm probes for the next available slot in the array and places the key-value pair there. This avoids the overhead of linked lists but can lead to clustering, where a series of collisions causes a long sequence of occupied slots.

### 3. Load Factor and Resizing

The **load factor** of a hash table is the ratio of the number of stored elements to the number of slots in the array. A high load factor increases the probability of collisions and can degrade performance.

To maintain a low load factor, hash tables are **resized** when the number of elements exceeds a certain threshold. This involves creating a new, larger array and re-hashing all the existing key-value pairs to their new locations. Resizing is an expensive operation, but it is necessary to maintain the hash table's average O(1) performance.

## Challenges with Massive Datasets

A single, massive hash table is often impractical for the following reasons:

*   **Memory:** A hash table with billions of entries would require an enormous amount of RAM on a single machine.
*   **Performance:** Resizing a massive hash table can be prohibitively slow.

To overcome these challenges, distributed systems use techniques like **Distributed Hash Tables (DHTs)** and **consistent hashing** to partition the data across a cluster of machines. Each machine is responsible for a subset of the keys, allowing the system to scale horizontally.

## Example: A Simple Hash Table with Chaining

Let's say we have a hash table of size 10 and we want to store the following key-value pairs:

*   `("apple", 5)`
*   `("banana", 8)`
*   `("orange", 3)`
*   `("grape", 12)`

Our hash function simply takes the length of the key string. So:

*   `hash("apple")` = 5
*   `hash("banana")` = 6
*   `hash("orange")` = 6
*   `hash("grape")` = 5

Our hash table would look like this:

*   Index 0: `null`
*   Index 1: `null`
*   Index 2: `null`
*   Index 3: `null`
*   Index 4: `null`
*   Index 5: `-> ("apple", 5) -> ("grape", 12)`
*   Index 6: `-> ("banana", 8) -> ("orange", 3)`
*   Index 7: `null`
*   Index 8: `null`
*   Index 9: `null`

Here, we have collisions at indices 5 and 6, which are resolved by creating linked lists.

# 6.1 The Streaming Model of Computation

The streaming model of computation is a paradigm for processing data that arrives as a continuous, and often massive, stream. In this model, algorithms are designed to process data in a single pass, using a limited amount of memory. This is in contrast to traditional batch processing, where the entire dataset is stored and processed at once.

## Key Characteristics

The streaming model is defined by a set of constraints that reflect the challenges of dealing with massive, high-velocity data:

*   **Single-Pass Processing:** Algorithms are designed to read the input data only once, in the order it arrives. This is a fundamental constraint because it is often impossible to store the entire dataset for multiple passes.

*   **Limited Memory:** The memory available to a streaming algorithm is significantly smaller than the size of the input data. The goal is to use sub-linear space, meaning the memory usage grows much slower than the size of the stream.

*   **Real-Time Processing:** Data is processed as it arrives, enabling timely insights and actions. This is crucial for applications that require immediate responses, such as fraud detection or real-time analytics.

*   **Approximate Results:** Due to the memory constraints, it is often not feasible to compute exact answers. Streaming algorithms frequently provide approximate results with provable error bounds. For many applications, a fast, approximate answer is more valuable than a slow, exact one.

## Common Problems and Algorithms

Streaming algorithms are used to solve a variety of problems that would be intractable with traditional methods. These algorithms often rely on the probabilistic data structures discussed in previous chapters.

*   **Frequency Estimation:** Counting the occurrences of specific elements in a stream. The **Count-Min Sketch** is a popular algorithm for this task.

*   **Distinct Element Counting:** Estimating the number of unique items in a stream. The **HyperLogLog** algorithm is a space-efficient solution for this problem.

*   **Heavy Hitters:** Identifying the most frequent elements in a stream.

*   **Sampling:** Maintaining a representative random sample of the data stream. **Reservoir Sampling** is a well-known technique for this.

*   **Sliding Window Operations:** Analyzing data over a fixed-size window of the most recent elements. For example, calculating the average value of a metric over the last 5 minutes.

## Example: Counting Unique Users

Imagine you are working for a popular social media platform and you want to count the number of unique users who log in each day. The number of daily active users can be in the billions, so storing the ID of every user who logs in would require a massive amount of memory.

In the streaming model, you would process a stream of login events. Each event contains a user ID. To count the number of unique users, you could use a **HyperLogLog** algorithm. As each login event arrives, you would add the user ID to the HLL structure. At the end of the day, the HLL would give you a highly accurate estimate of the number of unique users, while using only a few kilobytes of memory.

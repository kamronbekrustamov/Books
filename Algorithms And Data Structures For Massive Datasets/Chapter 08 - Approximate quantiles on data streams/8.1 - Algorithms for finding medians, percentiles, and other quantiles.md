# 8.1 Algorithms for Finding Medians, Percentiles, and Other Quantiles

Finding the median, percentiles, and other quantiles of a massive dataset is a common problem in data analysis. A quantile is the value below which a certain percentage of the data falls. For example, the median is the 50th percentile, meaning that 50% of the data is smaller than the median.

When a dataset is too large to fit into memory, it is impossible to find the exact quantiles in a single pass using traditional methods (which typically require sorting the data). Therefore, a class of "streaming" or "online" algorithms has been developed to compute approximate quantiles in a single pass with limited memory.

## The Challenge of Finding Quantiles in a Stream

The primary challenge of finding quantiles in a stream is that you cannot store all the data. You must process each element as it arrives and maintain a small "summary" of the data that can be used to estimate the quantiles.

## Common Algorithms

Here are some of the key algorithms for finding quantiles in massive datasets:

### 1. Random Sampling

One of the simplest approaches is to take a random sample of the dataset. The quantiles are then calculated on this smaller, more manageable sample. The accuracy of this method depends on the size of the sample; a larger sample will generally yield more accurate results.

### 2. Greenwald-Khanna (GK) Algorithm

The Greenwald-Khanna algorithm is a well-known deterministic streaming algorithm for computing approximate quantiles. It works by maintaining a summary data structure of the data stream. As new elements arrive, they are inserted into the summary, and a "compress" operation is periodically performed to merge elements and keep the summary size small. The GK algorithm provides a deterministic error guarantee, meaning the estimated quantile will be within a certain range of the true quantile.

### 3. Sketch-based Algorithms: Q-Digest and T-Digest

Sketching algorithms create a compact summary (a "sketch") of the data distribution, which can then be queried to get approximate quantiles.

*   **Q-Digest:** This is a tree-based data structure that is particularly effective when the dataset's values are drawn from a relatively small universe of possible values.
*   **T-Digest:** A more recent algorithm that also uses a tree-like structure. It is known for providing good estimates, especially for extreme quantiles (e.g., the 99th percentile).

### 4. KLL (Karnin, Lang, Liberty) Algorithm

The KLL algorithm is a randomized streaming algorithm that is considered asymptotically optimal in terms of space usage. It uses a data structure called a "compactor" to summarize the data stream. For a given error rate, the KLL algorithm uses a very low amount of memory, making it one of the most space-efficient algorithms for this problem.

## Example: Estimating the Median Latency of a Web Service

Imagine you are running a large web service and you want to monitor the median latency of all requests. The service handles millions of requests per minute, so you cannot store the latency of every single request.

To solve this problem, you can use a streaming quantile algorithm. As each request is processed, its latency is added to the algorithm's summary data structure. At any point in time, you can query the data structure to get an estimate of the median latency.

For example, you could use the **T-Digest** algorithm to maintain a sketch of the latency distribution. This would allow you to not only estimate the median (50th percentile) but also other important metrics like the 95th and 99th percentile latencies, which are often used to measure the performance of web services.
